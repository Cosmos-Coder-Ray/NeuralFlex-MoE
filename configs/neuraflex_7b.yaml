# NeuralFlex-MoE 7B Configuration

model:
  model_name: "NeuralFlex-MoE-7B"
  architecture: "Hybrid-MoE-Transformer"
  base_hidden_size: 2048
  num_attention_heads: 32
  num_key_value_heads: 8
  intermediate_size: 5632
  num_hidden_layers: 24
  vocab_size: 65536
  max_position_embeddings: 32768
  rope_theta: 500000.0
  sliding_window: 4096
  num_experts: 16
  num_experts_per_tok: 2
  expert_capacity_factor: 1.25

training:
  strategy: "FSDP"
  gradient_checkpointing: true
  mixed_precision: "bf16"
  gradient_accumulation_steps: 8
  micro_batch_size: 2
  optimizer: "AdamW-8bit"
  learning_rate: 3.0e-4
  warmup_steps: 2000
  total_steps: 100000
  eval_steps: 500
  save_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"

novel_features:
  self_organizing_pathways:
    enabled: true
    pathway_threshold: 0.01
    pruning_rate: 0.1
    growth_rate: 0.05
  
  temporal_context_compression:
    enabled: true
    compression_ratio: 10
    memory_bank_size: 10000
  
  uncertainty_aware_generation:
    enabled: true
    uncertainty_threshold: 0.7
    alternative_beams: 3
  
  continuous_learning:
    enabled: true
    experience_replay_size: 10000
    ewc_lambda: 0.4

data:
  datasets:
    - wikitext
    - openwebtext
    - github-code
    - gsm8k
  max_length: 32768
  streaming: true

deployment:
  quantization: "4bit"
  use_flash_attention: true
  use_kv_cache: true
  max_batch_size: 8
