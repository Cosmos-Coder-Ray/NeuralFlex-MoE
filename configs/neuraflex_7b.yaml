# NeuralFlex-MoE 7B Configuration

# Configuration for the model architecture itself
model_config:
  model_name: "NeuralFlex-MoE-7B"
  architecture: "Hybrid-MoE-Transformer"
  base_hidden_size: 2048
  num_attention_heads: 32
  num_key_value_heads: 8
  intermediate_size: 5632
  num_hidden_layers: 24
  vocab_size: 32000 # Adjusted to be closer to standard tokenizers like Llama
  max_position_embeddings: 32768
  rope_theta: 500000.0
  sliding_window: 4096
  num_experts: 16
  num_experts_per_tok: 2
  expert_capacity_factor: 1.25
  
  # Adaptive Reasoning Chain (TRM-inspired)
  arc_steps: 5
  arc_dropout: 0.1

# Configuration for the training process
training_config:
  output_dir: "./outputs/neuraflex-7b"
  tokenizer_name: "meta-llama/Llama-2-7b-hf"
  
  # Datasets
  train_dataset: "openhermes"
  eval_dataset: "gsm8k"

  # Core training parameters
  strategy: "FSDP"
  gradient_checkpointing: true
  mixed_precision: "bf16"
  gradient_accumulation_steps: 8
  micro_batch_size: 2
  optimizer: "AdamW-8bit"
  learning_rate: 3.0e-4
  warmup_steps: 2000
  total_steps: 100000
  eval_steps: 500
  save_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"

  # Weight Transfer from a base model
  use_weight_transfer: true
  source_model_id: "mistralai/Mistral-7B-v0.1"

  # Self-Organizing Neural Pathways (SONP)
  sonp_enabled: true
  sonp_update_frequency: 200 # Update pathways every 200 steps
  sonp_config:
    pathway_threshold: 0.01
    pruning_rate: 0.1
    growth_rate: 0.05

  # MoE specific loss
  moe_aux_loss_coeff: 0.01

# Deployment settings (not used in training script)
deployment:
  quantization: "4bit"
  use_flash_attention: true
  use_kv_cache: true
  max_batch_size: 8